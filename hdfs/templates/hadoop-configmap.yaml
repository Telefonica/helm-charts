apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "hadoop.fullname" . }}
  labels:
    app: {{ template "hadoop.name" . }}
    chart: {{ .Chart.Name }}-{{ .Chart.Version | replace "+" "_" }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  bootstrap.sh: |
    #!/bin/bash

    : ${HADOOP_PREFIX:=/usr/local/hadoop}

    . $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh

    # Directory to find config artifacts
    CONFIG_DIR="/tmp/hadoop-config"

    # Copy config files from volume mount

    for f in slaves core-site.xml hdfs-site.xml; do
      if [[ -e ${CONFIG_DIR}/$f ]]; then
        cp ${CONFIG_DIR}/$f $HADOOP_PREFIX/etc/hadoop/$f
      else
        echo "ERROR: Could not find $f in $CONFIG_DIR"
        exit 1
      fi
    done

    # installing libraries if any - (resource urls added comma separated to the ACP system variable)
    cd $HADOOP_PREFIX/share/hadoop/common ; for cp in ${ACP//,/ }; do  echo == $cp; curl -LO $cp ; done; cd -

    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
      mkdir -p /root/hdfs/namenode
      $HADOOP_PREFIX/bin/hdfs namenode -format -force -nonInteractive
      $HADOOP_PREFIX/sbin/hadoop-daemon.sh start namenode
    fi

    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
      mkdir -p /root/hdfs/datanode

      #  wait up to 30 seconds for namenode 
      (while [[ $count -lt 15 && -z `curl -sf http://{{ template "hadoop.fullname" . }}-hdfs-nn:50070` ]]; do ((count=count+1)) ; echo "Waiting for {{ template "hadoop.fullname" . }}-hdfs-nn" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for hdfs-nn, exiting." && exit 1

      $HADOOP_PREFIX/sbin/hadoop-daemon.sh start datanode
    fi

    if [[ $1 == "-d" ]]; then
      until find ${HADOOP_PREFIX}/logs -mmin -1 | egrep -q '.*'; echo "`date`: Waiting for logs..." ; do sleep 2 ; done
      tail -F ${HADOOP_PREFIX}/logs/* &
      while true; do sleep 1000; done
    fi

    if [[ $1 == "-bash" ]]; then
      /bin/bash
    fi

  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
            <name>fs.defaultFS</name>
            <value>hdfs://{{ template "hadoop.fullname" . }}-hdfs-nn:9000/</value>
            <description>NameNode URI</description>
        </property>
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>false</value>
      </property>

      <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>false</value>
      </property>

      <property>
        <name>dfs.replication</name>
          <value>3</value>
      </property>

      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///root/hdfs/datanode</value>
        <description>DataNode directory</description>
      </property>

      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///root/hdfs/namenode</value>
        <description>NameNode directory for namespace and transaction logs storage.</description>
      </property>

      <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
      </property>

      <!-- Bind to all interfaces -->
      <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.servicerpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->
      
    </configuration>

  slaves: |
    localhost
